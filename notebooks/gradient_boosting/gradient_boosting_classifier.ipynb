{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier\n",
    "This notebook defines the python script corresponding to the non-DL scenario. The multinomial classifier I used is the **XGB** implementation which supports the GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "XTrain = pd.read_csv('../../data/staging_data/mispelling_fixed_clean_input_train.csv', sep=',')\n",
    "YTrain = pd.read_csv('c:/Users/I051796/Projects/CES/data/label.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the text with TF-IDF transform\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents='ascii')\n",
    "XTFIDFVectorizedTrain = vectorizer.fit_transform(XTrain['question'])\n",
    "XTFIDFVectorizedTrain = pd.DataFrame(XTFIDFVectorizedTrain.toarray())\n",
    "XTFIDFVectorizedTrain.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a dimensional reduction into 600 dimensional space\n",
    "# use the same dimensionality than the one provided by fasttext embedding model in the DL context\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "PCATransform = PCA(n_components=600)\n",
    "PCAXTrain = pd.DataFrame(PCATransform.fit_transform(XTFIDFVectorizedTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# combine input features from above text vectorization with text characteristics features (../../data/staging_data/text_extracted_features.csv)\n",
    "extractedFeatures = pd.read_csv('../../data/staging_data/text_extracted_features.csv', sep=',')\n",
    "mergedXTrain = PCAXTrain.join(extractedFeatures, lsuffix='', rsuffix='')\n",
    "\n",
    "# split into train and test\n",
    "mergedXTrain, mergedXTest, YTrain, YTest = train_test_split(mergedXTrain, YTrain, test_size=0.15, random_state=42)\n",
    "\n",
    "mergedXTrain.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# run a grid search with key parameter value candidates\n",
    "# max_depth\n",
    "# min_child_weight\n",
    "# n_estimators\n",
    "# early stopping\n",
    "# learning_rate (eta)\n",
    "\n",
    "grid_parameters = {\n",
    "    'max_depth':(4,6,8),\n",
    "    \"min_child_weight\" :(2, 5, 10),\n",
    "    \"learning_rate\" :(0.05, 0.1)\n",
    "}\n",
    "\n",
    "gbm = xgb.XGBClassifier(\n",
    "    objective = \"multi:softprob\",\n",
    "    n_estimators=100,\n",
    "    eval_metric =\"mlogloss\",\n",
    "    n_jobs=2,    \n",
    "    tree_method='gpu_hist',\n",
    "    n_gpus=1)\n",
    "\n",
    "gridSearch = GridSearchCV(\n",
    "    estimator=gbm,\n",
    "    fit_params = None,\n",
    "    param_grid = grid_parameters,\n",
    "    cv=4,\n",
    "    verbose=1)\n",
    "\n",
    "gridSearch.fit(mergedXTrain, YTrain.intention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.DataFrame(gridSearch.cv_results_['mean_test_score']).plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search indicates that below parameters give the best accuracy:\n",
    "\n",
    "* min_child_weight=10\n",
    "* max_depth=8\n",
    "* learning_rate=0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "gbm = xgb.XGBClassifier(\n",
    "    min_child_weight=10,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    objective = \"multi:softprob\",\n",
    "    n_estimators=100,\n",
    "    eval_metric =\"mlogloss\",\n",
    "    n_jobs=2,\n",
    "    tree_method='gpu_hist',\n",
    "    n_gpus=1)\n",
    "\n",
    "\n",
    "gbm.fit(mergedXTrain, YTrain.intention, early_stopping_rounds=10, eval_set=[(mergedXTrain,YTrain.intention)], eval_metric='mlogloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(gbm.feature_importances_).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(gbm.evals_result_.get('validation_0').get('mlogloss')).plot(figsize=(12,8), title='logloss descent over boosting iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YPredicted = gbm.predict(mergedXTest)\n",
    "YTrue = YTest.intention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import vizu\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(YTrue, YPredicted)\n",
    "\n",
    "print(sklearn.metrics.classification_report(YTrue, YPredicted))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "vizu.plot_confusion_matrix(cnf_matrix, normalize=False, classes = np.unique(YTrain['intention']))\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
