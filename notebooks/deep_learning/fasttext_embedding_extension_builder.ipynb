{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vectorization with  FastText embedding model\n",
    "\n",
    "Facebook company supplies a general word embedding model at [https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)\n",
    "\n",
    "\n",
    "In case of **out of vocabulary** situation with Facebook's model, there are 2 strategies:\n",
    "*  if the word corresponds to **named entity** we consider as relevant w.r.t the classification, it's encoded into the vector of its hypernym/named entity which is \"known\" by the fasttext model\n",
    "  * all identified drug names => **'médicament'**\n",
    "  * all identified active ingrédient names => **'ingrédient'**\n",
    "  * all numerical tokens (eg: 10, 50, ..)=> **'nombre'**\n",
    "  * all time duration tokens (eg: 10 jours, ...) => **'durée'**\n",
    "  * all time duration tokens (eg: 10 jours, ...) => **'temps'**\n",
    "  * all time tokens (eg: 5h, ...) => **'temps'**\n",
    "  * all weight tokens (eg: 5mg, ...) => **'poids'**\n",
    "  * all volume tokens (eg: 5ml, ...) => **'volume'**\n",
    "* the fall-back is to encode remaining unknown words into a random numerical vector  \n",
    "\n",
    "we add some small vector variation based on the deterministic pseudo-random values initalized by the word hashing: it ensures that for isntance, drug names have the same random vector close to its named entity (\"médicament\")\n",
    "\n",
    "This specific vectorization scheme is persisted aside the Facebook model at [../../pretrained_models/custom_embedding_model.txt](../../pretrained_models/custom_embedding_model.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nbDims = 300 #len(embeddings_index['oui'])\n",
    "\n",
    "XTrain = pd.read_csv('../../data/staging_data/mispelling_fixed_clean_input_train.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1152466it [02:22, 8097.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import os, re, csv, math, codecs\n",
    "from tqdm import tqdm\n",
    "\n",
    "embeddings_index = {}\n",
    "f = codecs.open('../../pretrained_models/fasttext/wiki.fr.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the min/max values per dimension for the random vector\n",
    "if False: # switch to True to trigger the min/max recomputation    \n",
    "    import numpy as np\n",
    "    maxLimit = 100.\n",
    "    minValues = np.zeros(nbDims)\n",
    "    maxValues = np.zeros(nbDims)\n",
    "\n",
    "    init = True\n",
    "    for vector in embeddings_index.values():\n",
    "        for index, coord in enumerate(vector, start=0):\n",
    "                if init == True:\n",
    "                    ''' init values '''\n",
    "                    minValues[index] = coord\n",
    "                    if coord < maxLimit:\n",
    "                        maxValues[index] = coord                        \n",
    "                    init = False\n",
    "                else:\n",
    "                    if minValues[index] > coord:\n",
    "                        minValues[index] = coord\n",
    "                    if maxValues[index] < coord and coord < maxLimit: # skip outlier\n",
    "                        maxValues[index] = coord \n",
    "    # save the embedding space boundaries into a file to avoir intensive recomputation\n",
    "    pd.DataFrame(minValues).to_csv('../../data/staging_data/fasttext_minValues.txt', index=None, header=None)\n",
    "    pd.DataFrame(maxValues).to_csv('../../data/staging_data/fasttext_maxValues.txt', index=None, header=None)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxVectorValues = pd.read_csv('../../data/staging_data/fasttext_maxValues.txt', header=None)\n",
    "maxVectorValues = maxVectorValues[0].values\n",
    "\n",
    "minVectorValues = pd.read_csv('../../data/staging_data/fasttext_minValues.txt', header=None)\n",
    "minVectorValues = minVectorValues[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 10000\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(XTrain['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "vectorAmplitudes = np.zeros(nbDims)\n",
    "vectorAmplitudes = maxVectorValues - minVectorValues\n",
    "\n",
    "def addNoise2Vector(word, vector, randomVectorAmplitudes):\n",
    "    '''add noisy variation to the vector. The pseudo random generator is seeded with the word hash value\n",
    "    to guarantee that similar words have the same random vector\n",
    "    '''\n",
    "    v = np.array(vector)\n",
    "    randomVector = np.zeros(nbDims)\n",
    "    random.seed(word)\n",
    "    for dimIndex in range(0, nbDims):\n",
    "        randomVector[dimIndex] = random.uniform(-randomVectorAmplitudes[dimIndex],randomVectorAmplitudes[dimIndex])\n",
    "    v = v + randomVector\n",
    "    return v.tolist()    \n",
    "\n",
    "drugNames = Counter(words(open('../../data/staging_data/drug_names.txt').read()))\n",
    "ingredientNames = Counter(words(open('../../data/staging_data/ingredient_names.txt').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_embeddings = {}\n",
    "\n",
    "ingredientCount = 0\n",
    "drugCount = 0\n",
    "\n",
    "import random\n",
    "random.seed(1971)\n",
    "\n",
    "# 0.25% random variation\n",
    "randomVectorAmplitudes = vectorAmplitudes * 0.0025\n",
    "\n",
    "for word in tokenizer.word_index.keys():                \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is None or len(embedding_vector) == 0:\n",
    "        '''out of vocab'''\n",
    "        if word in drugNames:\n",
    "            custom_embeddings[word] = addNoise2Vector(word, embeddings_index.get('médicament'),randomVectorAmplitudes) \n",
    "            drugCount = drugCount + 1\n",
    "        elif word in ingredientNames:\n",
    "            custom_embeddings[word] = addNoise2Vector(word, embeddings_index.get('ingrédient'), randomVectorAmplitudes)\n",
    "            ingredientCount = ingredientCount + 1\n",
    "        else:\n",
    "            '''random vector'''\n",
    "            randomVector = np.zeros(nbDims)\n",
    "            for dimIndex in range(0, nbDims):\n",
    "                randomVector[dimIndex] = random.uniform(minVectorValues[dimIndex],maxVectorValues[dimIndex])            \n",
    "            custom_embeddings[word] = randomVector.tolist()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drugCount=673 ; ingredientCount=9 ; randomCount=796\n"
     ]
    }
   ],
   "source": [
    "randomCount = len(custom_embeddings) - drugCount - ingredientCount\n",
    "print(\"drugCount={0} ; ingredientCount={1} ; randomCount={2}\".format(drugCount, ingredientCount, randomCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half of out of vocabulary words are not encoded randomly: drug and ingredient entities are represented as semantically close data points in the embedding destination space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../utils/serializer.py\n",
    "\n",
    "import csv\n",
    "\n",
    "def saveEmbeddingVector(vectors, fileName):\n",
    "    ''' save a dict of numerical array'''\n",
    "    with open(fileName, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in vectors.items():\n",
    "            writer.writerow([key, \",\".join([str(i) for i in value])])\n",
    "    csv_file.close()\n",
    "\n",
    "def loadEmbeddingVector(fileName):\n",
    "    ''' load a dict of numerical array'''\n",
    "    with open(fileName, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file);\n",
    "        temp_dict = dict(reader)\n",
    "        myDict={k:list(map(lambda x: float(x), v.split(','))) for k,v in temp_dict.items()}    \n",
    "        csv_file.close()\n",
    "        return myDict \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embedding extension to the file system\n",
    "saveEmbeddingVector(custom_embeddings, '../../pretrained_models/fasttext_embedding_extension.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
