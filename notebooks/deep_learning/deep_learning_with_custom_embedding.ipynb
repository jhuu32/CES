{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Import libraries<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\apps\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keras\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "## Plot\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "import matplotlib as plt\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain = pd.read_csv('c:/Users/I051796/Projects/CES/data/input_train.csv', sep=';')\n",
    "YTrain = pd.read_csv('c:/Users/I051796/Projects/CES/data/label.csv', sep=';')\n",
    "num_classes = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\I051796\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "    \n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"french\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"\\?\\?\", \"?\", text)\n",
    "    text = re.sub(r\"([A-Z]|[a-z])'\", \"\", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('french')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain['question'] = XTrain['question'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "padding = 100\n",
    "embed_out_size = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(XTrain['question'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(XTrain['question'])\n",
    "XEncodedTrain = pad_sequences(sequences, maxlen=padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <b>Build neural network with LSTM<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 51)                5151      \n",
      "=================================================================\n",
      "Total params: 1,085,551\n",
      "Trainable params: 1,085,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto(device_count={\"CPU\": 32})\n",
    "keras.backend.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocabulary_size , embed_out_size, input_length=padding))\n",
    "model_lstm.add(LSTM(embed_out_size, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(num_classes, activation='softmax'))\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "YOneHotEncodedTrain = keras.utils.to_categorical(YTrain['intention'])\n",
    "print(YOneHotEncodedTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6422 samples, validate on 1606 samples\n",
      "Epoch 1/5\n",
      "6422/6422 [==============================] - 32s 5ms/step - loss: 3.1626 - acc: 0.2303 - val_loss: 2.7356 - val_acc: 0.2970\n",
      "Epoch 2/5\n",
      "6422/6422 [==============================] - 30s 5ms/step - loss: 2.3755 - acc: 0.4031 - val_loss: 2.1318 - val_acc: 0.4695\n",
      "Epoch 3/5\n",
      "6422/6422 [==============================] - 30s 5ms/step - loss: 1.6989 - acc: 0.5684 - val_loss: 1.8541 - val_acc: 0.5405\n",
      "Epoch 4/5\n",
      "6422/6422 [==============================] - 31s 5ms/step - loss: 1.2088 - acc: 0.6865 - val_loss: 1.7746 - val_acc: 0.5691\n",
      "Epoch 5/5\n",
      "6422/6422 [==============================] - 30s 5ms/step - loss: 0.8584 - acc: 0.7775 - val_loss: 1.7048 - val_acc: 0.5778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a036362b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.utils\n",
    "\n",
    "model_lstm.fit(XEncodedTrain, np.array(YOneHotEncodedTrain), validation_split=0.2, epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <b>Build neural network with LSTM and CNN <b>\n",
    "The LSTM model worked well. However, it takes forever to train three epochs. One way to speed up the training time is to improve the network adding “Convolutional” layer. Convolutional Neural Networks (CNN) come from image processing. They pass a “filter” over the data and calculate a higher-level representation. They have been shown to work surprisingly well for text, even though they have none of the sequence processing ability of LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Embedding(vocabulary_size, embed_out_size, input_length=padding))\n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(LSTM(100))\n",
    "    model_conv.add(Dense(num_classes, activation='softmax'))\n",
    "    model_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model_conv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6422 samples, validate on 1606 samples\n",
      "Epoch 1/5\n",
      "6422/6422 [==============================] - 19s 3ms/step - loss: 3.1259 - acc: 0.2323 - val_loss: 2.7453 - val_acc: 0.3144\n",
      "Epoch 2/5\n",
      "6422/6422 [==============================] - 18s 3ms/step - loss: 2.4915 - acc: 0.3543 - val_loss: 2.3624 - val_acc: 0.3811\n",
      "Epoch 3/5\n",
      "6422/6422 [==============================] - 18s 3ms/step - loss: 1.9807 - acc: 0.4611 - val_loss: 2.1826 - val_acc: 0.4259\n",
      "Epoch 4/5\n",
      "6422/6422 [==============================] - 18s 3ms/step - loss: 1.5813 - acc: 0.5740 - val_loss: 2.1112 - val_acc: 0.4738\n",
      "Epoch 5/5\n",
      "6422/6422 [==============================] - 18s 3ms/step - loss: 1.2706 - acc: 0.6587 - val_loss: 2.1119 - val_acc: 0.4819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a11c57860>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv = create_conv_model()\n",
    "model_conv.fit(XEncodedTrain, np.array(YOneHotEncodedTrain), validation_split=0.2, epochs = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
